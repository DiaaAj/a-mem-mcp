# Environment Configuration for Agentic Memory MCP Server
# Copy this file to .env and fill in your actual values

# ============================================================================
# LLM Backend Configuration
# ============================================================================

# LLM Backend: "openai", "ollama", "sglang", or "openrouter"
LLM_BACKEND=openai

# LLM Model Name
# - OpenAI: gpt-4o-mini, gpt-4, gpt-4-turbo, gpt-3.5-turbo
# - Ollama: llama2, mistral, codellama, etc.
# - SGLang: meta-llama/Llama-3.1-8B-Instruct, etc.
# - OpenRouter: openai/gpt-4o-mini, anthropic/claude-3.5-sonnet, etc.
LLM_MODEL=gpt-4o-mini

# ============================================================================
# API Keys (only set the one you need based on LLM_BACKEND)
# ============================================================================

# OpenAI API Key (for LLM_BACKEND=openai)
OPENAI_API_KEY=sk-your-openai-api-key-here

# OpenRouter API Key (for LLM_BACKEND=openrouter)
# OPENROUTER_API_KEY=sk-or-your-openrouter-key-here

# ============================================================================
# Embedding Configuration
# ============================================================================

# Sentence Transformer Model for Embeddings
# Popular options:
# - all-MiniLM-L6-v2 (default, fast and lightweight)
# - all-mpnet-base-v2 (better quality, slower)
# - multi-qa-mpnet-base-dot-v1 (optimized for Q&A)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# ============================================================================
# Memory Evolution Settings
# ============================================================================

# Number of memories before triggering evolution/consolidation
# Higher values = less frequent evolution but better performance
# Lower values = more frequent evolution but slower
EVO_THRESHOLD=100

# ============================================================================
# Storage Settings
# ============================================================================

# ChromaDB Storage Directory (where memory database is persisted)
# Default: ./chroma_db (relative to where the server is run)
# You can set an absolute path for persistent storage across sessions
# CHROMA_DB_PATH=./chroma_db

# ============================================================================
# SGLang Backend Settings (only needed if LLM_BACKEND=sglang)
# ============================================================================

# SGLang Server Host
# SGLANG_HOST=http://localhost

# SGLang Server Port
# SGLANG_PORT=30000

# ============================================================================
# MCP Server Settings
# ============================================================================

# Server Name (optional, defaults to "agentic-memory")
# SERVER_NAME=agentic-memory

# ============================================================================
# Usage Examples
# ============================================================================

# Example 1: OpenAI Configuration (recommended for getting started)
# LLM_BACKEND=openai
# LLM_MODEL=gpt-4o-mini
# OPENAI_API_KEY=sk-...
# EMBEDDING_MODEL=all-MiniLM-L6-v2
# EVO_THRESHOLD=100

# Example 2: Ollama Local Configuration (free, runs locally)
# LLM_BACKEND=ollama
# LLM_MODEL=llama2
# EMBEDDING_MODEL=all-MiniLM-L6-v2
# EVO_THRESHOLD=100

# Example 3: SGLang Fast Local Inference
# First start SGLang server:
#   python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct
# Then configure:
# LLM_BACKEND=sglang
# LLM_MODEL=meta-llama/Llama-3.1-8B-Instruct
# SGLANG_HOST=http://localhost
# SGLANG_PORT=30000
# EMBEDDING_MODEL=all-MiniLM-L6-v2
# EVO_THRESHOLD=100

# Example 4: OpenRouter (access to 100+ models)
# LLM_BACKEND=openrouter
# LLM_MODEL=openai/gpt-4o-mini
# OPENROUTER_API_KEY=sk-or-...
# EMBEDDING_MODEL=all-MiniLM-L6-v2
# EVO_THRESHOLD=100
